# -*- coding: utf-8 -*-
"""EMG_Robot_Arm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wzxUkUUD3WiubRubHHyrOlNdZzBxVgR9
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
#import sklearn

from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

from google.colab import drive
drive.mount('/content/drive')

from sklearn import svm

clf = svm.SVC()

df = pd.read_csv('/content/drive/MyDrive/After Midterm/EMG.csv')

#df.columns=['sensor','label']

df

df.describe()

DATA_LENGTH = df.count()[0]
NUMBER_OF_SAMPLES = 100
COLUMN_NAME = 'sensor'

def emg_extract(data,COLUMN_NAME,sampels):
  label=[]
  lmax=[]
  lmin=[]
  lvar=[]
  lmean=[]
  for i in range(0,data.count()[0],sampels):
    l=data[COLUMN_NAME][i:i+100] #iloc[:i+100,:1]#[:i+100]
    lmax.append(l.max())
    lvar.append(l.var())
    lmean.append(l.mean())
    lmin.append(l.min())
    label.append(data.iloc[i,-1])
  return lmax, lmin, lmean, lvar, label

l=emg_extract(df,COLUMN_NAME,NUMBER_OF_SAMPLES)

emg_data = pd.DataFrame({'Max' : l[0] ,'Min' : l[1] ,'Mean' : l[2] ,'Var' : l[3] ,'label' : l[4]})

emg_data

fig = plt.figure()
plt.plot(df.sensor)
plt.figure()
plt.plot(df.sensor[:5000],'y+')
plt.plot(df.sensor[5000:10000],'ro')
plt.plot(df.sensor[10000:15000],'gv')

from pathlib import Path  
filepath = Path('/content/drive/MyDrive/After Midterm/out.csv')  
filepath.parent.mkdir(parents=True, exist_ok=True)  
emg_data.to_csv(filepath)

clf.fit(np.array(emg_data.iloc[:,:-1]),np.array(emg_data.iloc[:,-1]))

clf.predict([np.array([209,	171, 193.52, 57.363232])])

test = emg_extract(df,COLUMN_NAME,10)

test_data = pd.DataFrame({'Max' : test[0] ,'Min' : test[1] ,'Mean' : test[2] ,'Var' : test[3] ,'label' : test[4]})

test_data

clf.fit(test_data.iloc[:,:-1],test_data.iloc[:,-1])

clf.predict(emg_data.iloc[:,:-1])

from keras.models import Sequential
from keras.layers import Dense,Conv1D, MaxPooling1D,Dropout,Reshape, GlobalAveragePooling1D
from keras.utils.vis_utils import plot_model
from keras.callbacks import ModelCheckpoint
from keras.utils import np_utils

model = Sequential() #model creation
model.add(Reshape((4,1),input_shape=(4,)))
model.add(Conv1D(50, 1, activation='relu', input_shape=(4,1)))
model.add(Conv1D(50, 1, activation='relu'))
model.add(MaxPooling1D(1))
model.add(Dense(50,activation='relu'))
model.add(Dense(50,activation='relu'))
model.add(GlobalAveragePooling1D())
model.add(Dense(3, activation='sigmoid'))
print(model.summary())

from keras.utils import to_categorical

x , x_test = test_data.iloc[:,:-1] , emg_data.iloc[:,:-1]

y , y_test = to_categorical(test_data.iloc[:,-1]) , to_categorical(emg_data.iloc[:,-1])

from sklearn.metrics import roc_curve

from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import confusion_matrix

model.compile(
    loss='categorical_crossentropy',
                optimizer='adam', metrics=['acc'])

# Hyper-parameters
BATCH_SIZE = 10
EPOCHS = 600

# Enable validation to use ModelCheckpoint and EarlyStopping callbacks.
history = model.fit(x,
                      y,
                      batch_size=BATCH_SIZE,
                      epochs=EPOCHS,
                      validation_split=0.2,
                      verbose=0
                      )

accuracy_results = model.evaluate(x_test, y_test)
print("Accuracy :",accuracy_results)

# %%

print("\n--- Learning curve of model training ---\n")

# summarize history for accuracy and loss
plt.figure(figsize=(6, 4))
plt.plot(history.history['acc'], "g--", label="Accuracy of training data")
plt.plot(history.history['val_acc'], "g", label="Accuracy of validation data")
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Training Epoch')
plt.legend()
plt.figure()
plt.plot(history.history['loss'], "r--", label="Loss of training data")
plt.plot(history.history['val_loss'], "r", label="Loss of validation data")
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Training Epoch')
#plt.ylim(0)
plt.legend()
plt.show()

#%%


# y_test = np_utils.to_categorical(y_test, num_classes)

# %%

print("\n--- Confusion matrix for test data ---\n")

y_pred_test = model.predict(x_test)
# # Take the class with the highest probability from the test predictions
max_y_pred_test = np.argmax(y_pred_test, axis=1)
max_y_test = np.argmax(y_test, axis=1)

print(confusion_matrix(max_y_test, max_y_pred_test))

# # %%

print("\n--- Classification report for test data ---\n")

# print(classification_report(max_y_test, max_y_pred_test))

#x_test, y_test = create_segments_and_labels(df_test,
#                                            TIME_PERIODS,
#                                            STEP_DISTANCE,
#                                           'LABEL')
#x_test = x_test.reshape(x_test.shape[0], input_shape)
#y_pred = model.predict(x_test)
# if accuracy_results[1]<0.5:
#   y_pred = 1-y_pred
#fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test,y_pred)
# plt.figure(1)
# plt.plot(fpr_keras, tpr_keras)
# plt.xlabel('False positive rate')
# plt.ylabel('True positive rate')
# plt.title('ROC curve')
# plt.legend(loc='best')
# plt.show()

y_pred = model.predict(x_test)

print(y_pred)

for a in range(6):
  print(np.argmax(y_pred[a]))

|